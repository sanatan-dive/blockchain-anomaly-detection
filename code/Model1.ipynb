{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOgmThVisoNZrD/hqI0m3ka",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanatan-dive/blockchain-anomaly-detection/blob/main/Model1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBF9QrujKFps",
        "outputId": "d4aed32a-9e59-49fb-ec17-1ce4d7ea79f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.86)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.6.15)\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.11/dist-packages (2.1.2+pt20cu118)\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.11/dist-packages (0.6.18+pt20cu118)\n",
            "Requirement already satisfied: torch-cluster in /usr/local/lib/python3.11/dist-packages (1.6.3+pt20cu118)\n",
            "Requirement already satisfied: torch-spline-conv in /usr/local/lib/python3.11/dist-packages (1.2.2+pt20cu118)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.15.3)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install torch-geometric\n",
        "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
        "!pip install scikit-learn pandas numpy matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, average_precision_score, confusion_matrix,\n",
        "    roc_curve, precision_recall_curve\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check GPU availability\n",
        "print(f\"🚀 Using device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "class BlockchainGCN(nn.Module):\n",
        "    \"\"\"\n",
        "    Graph Convolutional Network for blockchain anomaly detection\n",
        "    \"\"\"\n",
        "    def __init__(self, num_features, hidden_dim=64, num_classes=2, dropout=0.5):\n",
        "        super(BlockchainGCN, self).__init__()\n",
        "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.conv3 = GCNConv(hidden_dim, num_classes)\n",
        "        self.dropout = dropout\n",
        "        self.batch_norm1 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(hidden_dim)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # First GCN layer\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.batch_norm1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        # Second GCN layer\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.batch_norm2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        # Output layer\n",
        "        x = self.conv3(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "class BlockchainAnomalyDetector:\n",
        "    \"\"\"\n",
        "    Complete pipeline for blockchain anomaly detection using GNN\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim=64, dropout=0.5, lr=0.01, weight_decay=5e-4):\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.dropout = dropout\n",
        "        self.lr = lr\n",
        "        self.weight_decay = weight_decay\n",
        "        self.model = None\n",
        "        self.scaler = StandardScaler()\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"🔧 Detector initialized on {self.device}\")\n",
        "\n",
        "    def load_and_prepare_data(self, transactions_file='elliptic_txs_features.csv',\n",
        "                            edges_file='elliptic_txs_edgelist.csv',\n",
        "                            classes_file='elliptic_txs_classes.csv'):\n",
        "        \"\"\"\n",
        "        Load and prepare blockchain transaction data from Elliptic dataset\n",
        "        \"\"\"\n",
        "        print(\"🔄 Loading and preparing data...\")\n",
        "\n",
        "        # Load data\n",
        "        if transactions_file and edges_file and classes_file and os.path.exists(transactions_file) and os.path.exists(edges_file) and os.path.exists(classes_file):\n",
        "            try:\n",
        "                # Load transactions with header=None and set the first column as 'txId'\n",
        "                self.transactions = pd.read_csv(transactions_file, header=None)\n",
        "                self.transactions.columns = ['txId'] + [f'feature_{i}' for i in range(self.transactions.shape[1] - 1)]\n",
        "                self.edges = pd.read_csv(edges_file)\n",
        "                self.classes = pd.read_csv(classes_file)\n",
        "                print(f\"✅ Loaded {len(self.transactions)} transactions, {len(self.edges)} edges, and {len(self.classes)} class labels\")\n",
        "\n",
        "                # Debug: Print column names and detailed class labels\n",
        "                print(\"Transactions columns:\", self.transactions.columns.tolist())\n",
        "                print(\"Classes columns:\", self.classes.columns.tolist())\n",
        "                print(\"Sample class labels before merge (unique values):\", self.classes['class'].unique().tolist())\n",
        "                print(\"Sample class labels before merge (value counts):\", self.classes['class'].value_counts().to_dict())\n",
        "\n",
        "                # Identify the correct merge key\n",
        "                tx_id_col_transactions = 'txId'\n",
        "                tx_id_col_classes = 'txId' if 'txId' in self.classes.columns else self.classes.columns[0]\n",
        "\n",
        "                print(f\"Merging using: {tx_id_col_transactions} (transactions) and {tx_id_col_classes} (classes)\")\n",
        "\n",
        "                # Merge transactions with class labels using the identified columns\n",
        "                self.transactions = self.transactions.merge(\n",
        "                    self.classes,\n",
        "                    left_on=tx_id_col_transactions,\n",
        "                    right_on=tx_id_col_classes,\n",
        "                    how='left'\n",
        "                )\n",
        "\n",
        "                # Debug: Check class distribution after merge\n",
        "                print(\"Class distribution after merge (before filling):\", self.transactions['class'].value_counts().to_dict())\n",
        "\n",
        "                # Drop the duplicate ID column from classes if it exists and differs from 'txId'\n",
        "                if tx_id_col_classes != 'txId' and tx_id_col_classes in self.transactions.columns:\n",
        "                    self.transactions = self.transactions.drop(columns=[tx_id_col_classes])\n",
        "\n",
        "                # Verify the resulting columns\n",
        "                print(\"Merged transactions columns:\", self.transactions.columns.tolist())\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️  Error loading files: {e}\")\n",
        "                raise\n",
        "        else:\n",
        "            print(\"⚠️  Files not provided or not found. Please provide valid transaction, edge, and class files.\")\n",
        "            raise ValueError(\"Missing or invalid input files\")\n",
        "\n",
        "        # Data cleaning and preparation\n",
        "        self._clean_data()\n",
        "        self._prepare_features()\n",
        "        self._create_graph()\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _clean_data(self):\n",
        "        \"\"\"\n",
        "        Clean and validate the data\n",
        "        \"\"\"\n",
        "        print(\"🧹 Cleaning data...\")\n",
        "\n",
        "        # Remove duplicates using the 'txId' column\n",
        "        initial_count = len(self.transactions)\n",
        "        self.transactions = self.transactions.drop_duplicates(subset=['txId'])\n",
        "        self.edges = self.edges.drop_duplicates()\n",
        "\n",
        "        # Handle missing values (fill with median for numeric columns)\n",
        "        numeric_cols = self.transactions.select_dtypes(include=[np.number]).columns\n",
        "        self.transactions[numeric_cols] = self.transactions[numeric_cols].fillna(\n",
        "            self.transactions[numeric_cols].median()\n",
        "        )\n",
        "\n",
        "        # Only apply outlier removal if columns exist\n",
        "        for col in ['amount', 'gas_price', 'gas_limit']:\n",
        "            if col in self.transactions.columns:\n",
        "                Q1 = self.transactions[col].quantile(0.01)\n",
        "                Q3 = self.transactions[col].quantile(0.99)\n",
        "                self.transactions = self.transactions[\n",
        "                    (self.transactions[col] >= Q1) & (self.transactions[col] <= Q3)\n",
        "                ]\n",
        "\n",
        "        print(f\"✅ Data cleaned: {initial_count} → {len(self.transactions)} transactions\")\n",
        "\n",
        "    def _prepare_features(self):\n",
        "        \"\"\"\n",
        "        Prepare features for the GNN model\n",
        "        \"\"\"\n",
        "        print(\"🔧 Preparing features...\")\n",
        "\n",
        "        # Handle NaN in class column\n",
        "        if self.transactions['class'].isna().any():\n",
        "            self.transactions['class'] = self.transactions['class'].fillna('unknown')\n",
        "            print(f\"⚠️ Filled NaN class labels with 'unknown'.\")\n",
        "\n",
        "        # Convert class column to string for consistent mapping\n",
        "        self.transactions['class'] = self.transactions['class'].astype(str)\n",
        "\n",
        "        # Enhanced class mapping to handle various formats\n",
        "        class_mapping = {\n",
        "            'unknown': -1,\n",
        "            'licit': 0,\n",
        "            'illicit': 1,\n",
        "            '1': 0,  # Assuming '1' means licit in Elliptic dataset\n",
        "            '2': 1,  # Assuming '2' means illicit in Elliptic dataset\n",
        "            '-1': -1,\n",
        "            '-1.0': -1,\n",
        "            'nan': -1\n",
        "        }\n",
        "\n",
        "        # Map class labels\n",
        "        self.transactions['class'] = self.transactions['class'].map(class_mapping)\n",
        "\n",
        "        # Drop rows where mapping failed (if any)\n",
        "        unmapped = self.transactions['class'].isna()\n",
        "        if unmapped.any():\n",
        "            print(f\"⚠️ Found {unmapped.sum()} rows with unmapped class labels. Dropping them.\")\n",
        "            print(f\"Unmapped values: {self.transactions.loc[unmapped, 'class'].unique()}\")\n",
        "            self.transactions = self.transactions[~unmapped]\n",
        "\n",
        "        # Select features (exclude non-numeric columns)\n",
        "        feature_cols = [col for col in self.transactions.columns\n",
        "                       if col not in ['txId', 'class', 'time_step'] and\n",
        "                       self.transactions[col].dtype in [np.number, 'float64', 'int64']]\n",
        "\n",
        "        if len(feature_cols) == 0:\n",
        "            raise ValueError(\"No numeric feature columns found!\")\n",
        "\n",
        "        self.X = self.transactions[feature_cols].values\n",
        "        self.y = self.transactions['class'].values\n",
        "\n",
        "        # Filter out unknown class (-1) and store known_mask\n",
        "        self.known_mask = (self.y != -1)\n",
        "        if not self.known_mask.any():\n",
        "            raise ValueError(\"❌ No known labels (0 or 1) found in the dataset.\")\n",
        "\n",
        "        print(f\"📊 Before filtering: {len(self.y)} samples\")\n",
        "        print(f\"📊 After filtering unknown: {self.known_mask.sum()} samples\")\n",
        "\n",
        "        self.X = self.X[self.known_mask]\n",
        "        self.y = self.y[self.known_mask]\n",
        "\n",
        "        # Handle infinite values\n",
        "        self.X = np.nan_to_num(self.X, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        # Normalize features\n",
        "        self.X = self.scaler.fit_transform(self.X)\n",
        "\n",
        "        # Convert to tensors\n",
        "        self.X = torch.FloatTensor(self.X)\n",
        "        self.y = torch.LongTensor(self.y)\n",
        "\n",
        "        # Compute class weights for handling imbalanced data\n",
        "        class_counts = Counter(self.y.numpy())\n",
        "        total = len(self.y)\n",
        "        self.class_weights = torch.FloatTensor([\n",
        "            total / (2 * class_counts.get(0, 1)),\n",
        "            total / (2 * class_counts.get(1, 1))\n",
        "        ]).to(self.device)\n",
        "\n",
        "        print(f\"📊 Final class distribution: {dict(class_counts)}\")\n",
        "        print(f\"⚖️ Class weights: {self.class_weights.cpu().numpy()}\")\n",
        "        print(f\"📐 Feature matrix shape: {self.X.shape}\")\n",
        "\n",
        "    def _create_graph(self):\n",
        "        \"\"\"\n",
        "        Create PyTorch Geometric graph data\n",
        "        \"\"\"\n",
        "        print(\"🕸️  Creating graph structure...\")\n",
        "\n",
        "        # Get known txIds from the original txId column\n",
        "        known_tx_ids = self.transactions.loc[self.known_mask, 'txId'].values\n",
        "        tx_id_to_idx = {tx_id: idx for idx, tx_id in enumerate(known_tx_ids)}\n",
        "\n",
        "        # Debug: Check the number of known txIds and their presence in edges\n",
        "        print(f\"Debug: Number of known txIds: {len(known_tx_ids)}\")\n",
        "\n",
        "        # Check if edges file has the expected columns\n",
        "        if 'txId1' not in self.edges.columns or 'txId2' not in self.edges.columns:\n",
        "            # Try alternative column names\n",
        "            edge_cols = self.edges.columns.tolist()\n",
        "            if len(edge_cols) >= 2:\n",
        "                self.edges = self.edges.rename(columns={edge_cols[0]: 'txId1', edge_cols[1]: 'txId2'})\n",
        "            else:\n",
        "                raise ValueError(\"Edge file must have at least 2 columns for source and target nodes\")\n",
        "\n",
        "        edge_tx_ids = set(self.edges['txId1']).union(set(self.edges['txId2']))\n",
        "        known_edge_overlap = len(set(known_tx_ids).intersection(edge_tx_ids))\n",
        "        print(f\"Debug: Number of known txIds overlapping with edges: {known_edge_overlap}\")\n",
        "\n",
        "        # Filter edges to include only those with known txIds\n",
        "        valid_edges = []\n",
        "        for tx1, tx2 in zip(self.edges['txId1'], self.edges['txId2']):\n",
        "            if tx1 in tx_id_to_idx and tx2 in tx_id_to_idx:\n",
        "                valid_edges.append((tx_id_to_idx[tx1], tx_id_to_idx[tx2]))\n",
        "\n",
        "        if not valid_edges:\n",
        "            print(\"⚠️ No valid edges found. Creating a simple sequential graph structure.\")\n",
        "            # Create a simple chain graph if no edges are available\n",
        "            valid_edges = [(i, i+1) for i in range(len(known_tx_ids)-1)]\n",
        "\n",
        "        # Convert to undirected graph by adding reverse edges\n",
        "        undirected_edges = []\n",
        "        for edge in valid_edges:\n",
        "            undirected_edges.append(edge)\n",
        "            undirected_edges.append((edge[1], edge[0]))  # Add reverse edge\n",
        "\n",
        "        edge_index = torch.LongTensor(undirected_edges).t().contiguous()\n",
        "\n",
        "        # Move tensors to device\n",
        "        x = self.X.to(self.device)\n",
        "        y = self.y.to(self.device)\n",
        "        edge_index = edge_index.to(self.device)\n",
        "\n",
        "        # Create train/validation/test masks with stratified split\n",
        "        known_indices = np.arange(len(self.y))\n",
        "\n",
        "        # Ensure we have enough samples for splitting\n",
        "        if len(known_indices) < 10:\n",
        "            raise ValueError(\"Not enough samples for train/validation/test split\")\n",
        "\n",
        "        train_idx, temp_idx = train_test_split(\n",
        "            known_indices,\n",
        "            test_size=0.4,\n",
        "            stratify=self.y,\n",
        "            random_state=42\n",
        "        )\n",
        "        val_idx, test_idx = train_test_split(\n",
        "            temp_idx,\n",
        "            test_size=0.5,\n",
        "            stratify=self.y[temp_idx],\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        # Create boolean masks\n",
        "        train_mask = torch.zeros(len(self.y), dtype=torch.bool, device=self.device)\n",
        "        val_mask = torch.zeros(len(self.y), dtype=torch.bool, device=self.device)\n",
        "        test_mask = torch.zeros(len(self.y), dtype=torch.bool, device=self.device)\n",
        "\n",
        "        train_mask[train_idx] = True\n",
        "        val_mask[val_idx] = True\n",
        "        test_mask[test_idx] = True\n",
        "\n",
        "        # Create PyG Data object\n",
        "        self.data = Data(\n",
        "            x=x, y=y, edge_index=edge_index,\n",
        "            train_mask=train_mask, val_mask=val_mask, test_mask=test_mask\n",
        "        )\n",
        "\n",
        "        print(f\"✅ Graph created: {self.data.num_nodes} nodes, {self.data.num_edges} edges\")\n",
        "        print(f\"   Train: {train_mask.sum()}, Val: {val_mask.sum()}, Test: {test_mask.sum()}\")\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"\n",
        "        Build the GCN model\n",
        "        \"\"\"\n",
        "        print(\"🏗️  Building GCN model...\")\n",
        "\n",
        "        self.model = BlockchainGCN(\n",
        "            num_features=self.data.num_node_features,\n",
        "            hidden_dim=self.hidden_dim,\n",
        "            dropout=self.dropout\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.optimizer = Adam(\n",
        "            self.model.parameters(),\n",
        "            lr=self.lr,\n",
        "            weight_decay=self.weight_decay\n",
        "        )\n",
        "\n",
        "        total_params = sum(p.numel() for p in self.model.parameters())\n",
        "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "\n",
        "        print(f\"✅ Model built:\")\n",
        "        print(f\"   Total parameters: {total_params:,}\")\n",
        "        print(f\"   Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def train(self, epochs=300, verbose=True):\n",
        "        \"\"\"\n",
        "        Train the GCN model\n",
        "        \"\"\"\n",
        "        print(\"🏋️‍♂️ Starting training...\")\n",
        "\n",
        "        self.model.train()\n",
        "        criterion = torch.nn.CrossEntropyLoss(weight=self.class_weights)\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Training step\n",
        "            self.model.train()\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            out = self.model(self.data.x, self.data.edge_index)\n",
        "            loss = criterion(out[self.data.train_mask], self.data.y[self.data.train_mask])\n",
        "\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "            # Validation step\n",
        "            self.model.eval()\n",
        "            with torch.no_grad():\n",
        "                val_out = self.model(self.data.x, self.data.edge_index)\n",
        "                val_loss = criterion(val_out[self.data.val_mask], self.data.y[self.data.val_mask])\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            if verbose and (epoch + 1) % 50 == 0:\n",
        "                print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n",
        "\n",
        "        print(\"✅ Training completed!\")\n",
        "\n",
        "        # Save model\n",
        "        torch.save(self.model.state_dict(), '/content/best_gnn_model.pth')\n",
        "        print(\"💾 Model saved to '/content/best_gnn_model.pth'\")\n",
        "\n",
        "        return train_losses, val_losses\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Comprehensive evaluation of the model\n",
        "        \"\"\"\n",
        "        print(\"📊 Evaluating model...\")\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Get predictions\n",
        "            out = self.model(self.data.x, self.data.edge_index)\n",
        "            pred_probs = torch.exp(out)  # Convert log probabilities to probabilities\n",
        "            pred_labels = out.argmax(dim=1)\n",
        "\n",
        "            # Test set evaluation\n",
        "            test_mask = self.data.test_mask\n",
        "            y_true = self.data.y[test_mask].cpu().numpy()\n",
        "            y_pred = pred_labels[test_mask].cpu().numpy()\n",
        "            y_prob = pred_probs[test_mask, 1].cpu().numpy()  # Probability of illicit class\n",
        "\n",
        "        # Calculate metrics\n",
        "        metrics = {\n",
        "            'accuracy': accuracy_score(y_true, y_pred),\n",
        "            'precision': precision_score(y_true, y_pred, zero_division=0),\n",
        "            'recall': recall_score(y_true, y_pred, zero_division=0),\n",
        "            'f1': f1_score(y_true, y_pred, zero_division=0),\n",
        "            'roc_auc': roc_auc_score(y_true, y_prob) if len(np.unique(y_true)) > 1 else np.nan,\n",
        "            'pr_auc': average_precision_score(y_true, y_prob) if len(np.unique(y_true)) > 1 else np.nan\n",
        "        }\n",
        "\n",
        "        # Print results\n",
        "        print(\"\\n🎯 Model Performance:\")\n",
        "        print(\"=\" * 40)\n",
        "        for metric, value in metrics.items():\n",
        "            if np.isnan(value):\n",
        "                print(f\"{metric.upper():>12}: NaN\")\n",
        "            else:\n",
        "                print(f\"{metric.upper():>12}: {value:.4f}\")\n",
        "\n",
        "        # Confusion matrix\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        print(f\"\\n🔍 Confusion Matrix:\")\n",
        "        print(\"=\" * 25)\n",
        "        if cm.shape == (2, 2):\n",
        "            print(f\"True Negative:  {cm[0,0]:4d} | False Positive: {cm[0,1]:4d}\")\n",
        "            print(f\"False Negative: {cm[1,0]:4d} | True Positive:  {cm[1,1]:4d}\")\n",
        "        else:\n",
        "            print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "        # Save predictions\n",
        "        self._save_predictions(y_true, y_pred, y_prob)\n",
        "\n",
        "        return metrics, y_true, y_pred, y_prob\n",
        "\n",
        "    def _save_predictions(self, y_true, y_pred, y_prob):\n",
        "        \"\"\"\n",
        "        Save predictions to CSV\n",
        "        \"\"\"\n",
        "        test_indices = torch.where(self.data.test_mask)[0].cpu().numpy()\n",
        "\n",
        "        results_df = pd.DataFrame({\n",
        "            'transaction_id': test_indices,\n",
        "            'true_label': y_true,\n",
        "            'predicted_label': y_pred,\n",
        "            'illicit_probability': y_prob,\n",
        "            'is_flagged': (y_prob > 0.5).astype(int),\n",
        "            'confidence': np.maximum(y_prob, 1 - y_prob)\n",
        "        })\n",
        "\n",
        "        # Sort by illicit probability (most suspicious first)\n",
        "        results_df = results_df.sort_values('illicit_probability', ascending=False)\n",
        "\n",
        "        results_df.to_csv('/content/gnn_flagged_transactions.csv', index=False)\n",
        "        print(\"💾 Predictions saved to '/content/gnn_flagged_transactions.csv'\")\n",
        "\n",
        "        # Show top 10 most suspicious transactions\n",
        "        print(\"\\n🚨 Top 10 Most Suspicious Transactions:\")\n",
        "        print(results_df.head(10)[['transaction_id', 'true_label', 'illicit_probability', 'is_flagged']])\n",
        "\n",
        "    def visualize_results(self, y_true, y_pred, y_prob):\n",
        "        \"\"\"\n",
        "        Create individual visualizations for model evaluation\n",
        "        \"\"\"\n",
        "        print(\"📈 Creating visualizations...\")\n",
        "\n",
        "        # Check if we have valid data for visualization\n",
        "        if len(np.unique(y_true)) < 2:\n",
        "            print(\"⚠️ Cannot create ROC/PR curves: only one class present in test set\")\n",
        "            # Still create other plots that don't require both classes\n",
        "\n",
        "        # 1. ROC Curve\n",
        "        if len(np.unique(y_true)) > 1:\n",
        "            plt.figure(figsize=(8, 6))\n",
        "            try:\n",
        "                fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
        "                roc_auc = roc_auc_score(y_true, y_prob)\n",
        "                plt.plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
        "                plt.plot([0, 1], [0, 1], 'k--', alpha=0.6, label='Random Classifier')\n",
        "                plt.fill_between(fpr, tpr, alpha=0.2)\n",
        "                plt.xlabel('False Positive Rate')\n",
        "                plt.ylabel('True Positive Rate')\n",
        "                plt.title('🎯 ROC Curve')\n",
        "                plt.legend()\n",
        "                plt.grid(True, alpha=0.3)\n",
        "                plt.savefig('/content/roc_curve.png', dpi=300, bbox_inches='tight')\n",
        "                plt.close()\n",
        "            except Exception as e:\n",
        "                print(f\"Error plotting ROC Curve: {e}\")\n",
        "        else:\n",
        "            print(\"Skipping ROC Curve due to single class in test set\")\n",
        "\n",
        "        # 2. Precision-Recall Curve\n",
        "        if len(np.unique(y_true)) > 1:\n",
        "            plt.figure(figsize=(8, 6))\n",
        "            try:\n",
        "                precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
        "                pr_auc = average_precision_score(y_true, y_prob)\n",
        "                plt.plot(recall, precision, linewidth=2, label=f'PR Curve (AUC = {pr_auc:.3f})')\n",
        "                plt.fill_between(recall, precision, alpha=0.2)\n",
        "                plt.xlabel('Recall')\n",
        "                plt.ylabel('Precision')\n",
        "                plt.title('📊 Precision-Recall Curve')\n",
        "                plt.legend()\n",
        "                plt.grid(True, alpha=0.3)\n",
        "                plt.savefig('/content/precision_recall_curve.png', dpi=300, bbox_inches='tight')\n",
        "                plt.close()\n",
        "            except Exception as e:\n",
        "                print(f\"Error plotting Precision-Recall Curve: {e}\")\n",
        "        else:\n",
        "            print(\"Skipping Precision-Recall Curve due to single class in test set\")\n",
        "\n",
        "        # 3. Confusion Matrix\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        if cm.size > 0:\n",
        "            cm_normalized = cm.astype('float') / (cm.sum(axis=1)[:, np.newaxis] + 1e-8)\n",
        "            sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',\n",
        "                        xticklabels=['Licit', 'Illicit'], yticklabels=['Licit', 'Illicit'])\n",
        "            plt.title('🔍 Confusion Matrix (Normalized)')\n",
        "            plt.xlabel('Predicted')\n",
        "            plt.ylabel('Actual')\n",
        "            plt.savefig('/content/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "        # 4. Prediction Distribution\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        if len(np.unique(y_true)) > 1:\n",
        "            plt.hist(y_prob[y_true == 0], bins=30, alpha=0.7, label='Licit', density=True, color='skyblue')\n",
        "            plt.hist(y_prob[y_true == 1], bins=30, alpha=0.7, label='Illicit', density=True, color='salmon')\n",
        "        else:\n",
        "            unique_class = np.unique(y_true)[0]\n",
        "            plt.hist(y_prob, bins=30, alpha=0.7, label=f'Class {unique_class}', density=True, color='gray')\n",
        "        plt.axvline(x=0.5, color='red', linestyle='--', alpha=0.8, label='Decision Threshold')\n",
        "        plt.xlabel('Predicted Probability (Illicit)')\n",
        "        plt.ylabel('Density')\n",
        "        plt.title('📈 Prediction Probability Distribution')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.savefig('/content/prediction_distribution.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        # 5. Feature Importance\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        try:\n",
        "            # Extract weights from the first GCN layer as a proxy for feature importance\n",
        "            self.model.eval()\n",
        "            with torch.no_grad():\n",
        "                weights = self.model.conv1.lin.weight.data.cpu().numpy()\n",
        "                # Compute absolute mean weights across output features\n",
        "                feature_importance = np.mean(np.abs(weights), axis=0)\n",
        "                # Normalize to sum to 1 for relative importance\n",
        "                feature_importance = feature_importance / np.sum(feature_importance)\n",
        "\n",
        "            # Get feature names from the original dataset\n",
        "            feature_cols = [col for col in self.transactions.columns\n",
        "                          if col not in ['txId', 'class', 'time_step'] and\n",
        "                          self.transactions[col].dtype in [np.number, 'float64', 'int64']]\n",
        "            feature_names = feature_cols[:len(feature_importance)]  # Match length of importance scores\n",
        "\n",
        "            # Create bar plot\n",
        "            plt.bar(range(len(feature_importance)), feature_importance, color='teal')\n",
        "            plt.xlabel('Feature Index')\n",
        "            plt.ylabel('Relative Importance')\n",
        "            plt.title('🎯 Feature Importance')\n",
        "            plt.xticks(range(len(feature_importance)), feature_names, rotation=90, fontsize=8)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig('/content/feature_importance.png', dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "        except Exception as e:\n",
        "            print(f\"Error plotting Feature Importance: {e}\")\n",
        "            plt.text(0.5, 0.5, 'Feature Importance\\n(Computation Failed)',\n",
        "                    ha='center', va='center', fontsize=12)\n",
        "            plt.title('🎯 Feature Importance')\n",
        "            plt.axis('off')\n",
        "            plt.savefig('/content/feature_importance.png', dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "        # 6. Performance Metrics Bar Chart\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "        metrics_values = [\n",
        "            accuracy_score(y_true, y_pred),\n",
        "            precision_score(y_true, y_pred, zero_division=0),\n",
        "            recall_score(y_true, y_pred, zero_division=0),\n",
        "            f1_score(y_true, y_pred, zero_division=0)\n",
        "        ]\n",
        "        if len(np.unique(y_true)) > 1:\n",
        "            metrics_names.extend(['ROC-AUC', 'PR-AUC'])\n",
        "            metrics_values.extend([\n",
        "                roc_auc_score(y_true, y_prob),\n",
        "                average_precision_score(y_true, y_prob)\n",
        "            ])\n",
        "        colors = plt.cm.Set3(np.linspace(0, 1, len(metrics_names)))\n",
        "        bars = plt.bar(metrics_names, metrics_values, color=colors)\n",
        "        plt.ylim(0, 1)\n",
        "        plt.title('📋 Performance Metrics Summary')\n",
        "        plt.ylabel('Score')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        for bar, value in zip(bars, metrics_values):\n",
        "            if not np.isnan(value):\n",
        "                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                        f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('/content/performance_metrics.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        print(\"💾 Individual visualizations saved to '/content/':\")\n",
        "        print(\"   - roc_curve.png (if applicable)\")\n",
        "        print(\"   - precision_recall_curve.png (if applicable)\")\n",
        "        print(\"   - confusion_matrix.png\")\n",
        "        print(\"   - prediction_distribution.png\")\n",
        "        print(\"   - feature_importance.png\")\n",
        "        print(\"   - performance_metrics.png\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main execution function\n",
        "    \"\"\"\n",
        "    print(\"🚀 Starting Blockchain Anomaly Detection Pipeline\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        # Initialize detector\n",
        "        detector = BlockchainAnomalyDetector(\n",
        "            hidden_dim=128,\n",
        "            dropout=0.3,\n",
        "            lr=0.005,\n",
        "            weight_decay=5e-4\n",
        "        )\n",
        "\n",
        "        # Load and prepare data\n",
        "        detector.load_and_prepare_data()\n",
        "\n",
        "        # Build model\n",
        "        detector.build_model()\n",
        "\n",
        "        # Train model\n",
        "        train_losses, val_losses = detector.train(epochs=200)\n",
        "\n",
        "        # Evaluate model\n",
        "        metrics, y_true, y_pred, y_prob = detector.evaluate()\n",
        "\n",
        "        # Create visualizations\n",
        "        detector.visualize_results(y_true, y_pred, y_prob)\n",
        "\n",
        "        print(\"\\n🎉 Pipeline completed successfully!\")\n",
        "        print(\"\\nFiles created:\")\n",
        "        print(\"- /content/best_gnn_model.pth\")\n",
        "        print(\"- /content/gnn_flagged_transactions.csv\")\n",
        "        print(\"- /content/gnn_evaluation_results.png\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error in pipeline: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "    # Download files to local machine\n",
        "    print(\"\\n💾 Files available for download:\")\n",
        "    print(\"   - /content/best_gnn_model.pth (trained model)\")\n",
        "    print(\"   - /content/gnn_flagged_transactions.csv (predictions)\")\n",
        "    print(\"   - /content/roc_curve.png (ROC Curve, if applicable)\")\n",
        "    print(\"   - /content/precision_recall_curve.png (Precision-Recall Curve, if applicable)\")\n",
        "    print(\"   - /content/confusion_matrix.png (Confusion Matrix)\")\n",
        "    print(\"   - /content/prediction_distribution.png (Prediction Distribution)\")\n",
        "    print(\"   - /content/feature_importance.png (Feature Importance)\")\n",
        "    print(\"   - /content/performance_metrics.png (Performance Metrics)\")\n",
        "\n",
        "    # Uncomment to download files\n",
        "    # files.download('/content/best_gnn_model.pth')\n",
        "    # files.download('/content/gnn_flagged_transactions.csv')\n",
        "    # files.download('/content/roc_curve.png')\n",
        "    # files.download('/content/precision_recall_curve.png')\n",
        "    # files.download('/content/confusion_matrix.png')\n",
        "    # files.download('/content/prediction_distribution.png')\n",
        "    # files.download('/content/feature_importance.png')\n",
        "    # files.download('/content/performance_metrics.png')"
      ],
      "metadata": {
        "id": "rrcu1u9PaXcu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e63315b4-af8c-4e17-80e7-604f9eef21f2"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Using device: cuda\n",
            "   GPU: Tesla T4\n",
            "🚀 Starting Blockchain Anomaly Detection Pipeline\n",
            "============================================================\n",
            "🔧 Detector initialized on cuda\n",
            "🔄 Loading and preparing data...\n",
            "✅ Loaded 203769 transactions, 234355 edges, and 203769 class labels\n",
            "Transactions columns: ['txId', 'feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4', 'feature_5', 'feature_6', 'feature_7', 'feature_8', 'feature_9', 'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14', 'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19', 'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24', 'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29', 'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34', 'feature_35', 'feature_36', 'feature_37', 'feature_38', 'feature_39', 'feature_40', 'feature_41', 'feature_42', 'feature_43', 'feature_44', 'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49', 'feature_50', 'feature_51', 'feature_52', 'feature_53', 'feature_54', 'feature_55', 'feature_56', 'feature_57', 'feature_58', 'feature_59', 'feature_60', 'feature_61', 'feature_62', 'feature_63', 'feature_64', 'feature_65', 'feature_66', 'feature_67', 'feature_68', 'feature_69', 'feature_70', 'feature_71', 'feature_72', 'feature_73', 'feature_74', 'feature_75', 'feature_76', 'feature_77', 'feature_78', 'feature_79', 'feature_80', 'feature_81', 'feature_82', 'feature_83', 'feature_84', 'feature_85', 'feature_86', 'feature_87', 'feature_88', 'feature_89', 'feature_90', 'feature_91', 'feature_92', 'feature_93', 'feature_94', 'feature_95', 'feature_96', 'feature_97', 'feature_98', 'feature_99', 'feature_100', 'feature_101', 'feature_102', 'feature_103', 'feature_104', 'feature_105', 'feature_106', 'feature_107', 'feature_108', 'feature_109', 'feature_110', 'feature_111', 'feature_112', 'feature_113', 'feature_114', 'feature_115', 'feature_116', 'feature_117', 'feature_118', 'feature_119', 'feature_120', 'feature_121', 'feature_122', 'feature_123', 'feature_124', 'feature_125', 'feature_126', 'feature_127', 'feature_128', 'feature_129', 'feature_130', 'feature_131', 'feature_132', 'feature_133', 'feature_134', 'feature_135', 'feature_136', 'feature_137', 'feature_138', 'feature_139', 'feature_140', 'feature_141', 'feature_142', 'feature_143', 'feature_144', 'feature_145', 'feature_146', 'feature_147', 'feature_148', 'feature_149', 'feature_150', 'feature_151', 'feature_152', 'feature_153', 'feature_154', 'feature_155', 'feature_156', 'feature_157', 'feature_158', 'feature_159', 'feature_160', 'feature_161', 'feature_162', 'feature_163', 'feature_164', 'feature_165']\n",
            "Classes columns: ['txId', 'class']\n",
            "Sample class labels before merge (unique values): ['unknown', '2', '1']\n",
            "Sample class labels before merge (value counts): {'unknown': 157205, '2': 42019, '1': 4545}\n",
            "Merging using: txId (transactions) and txId (classes)\n",
            "Class distribution after merge (before filling): {'unknown': 157205, '2': 42019, '1': 4545}\n",
            "Merged transactions columns: ['txId', 'feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4', 'feature_5', 'feature_6', 'feature_7', 'feature_8', 'feature_9', 'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14', 'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19', 'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24', 'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29', 'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34', 'feature_35', 'feature_36', 'feature_37', 'feature_38', 'feature_39', 'feature_40', 'feature_41', 'feature_42', 'feature_43', 'feature_44', 'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49', 'feature_50', 'feature_51', 'feature_52', 'feature_53', 'feature_54', 'feature_55', 'feature_56', 'feature_57', 'feature_58', 'feature_59', 'feature_60', 'feature_61', 'feature_62', 'feature_63', 'feature_64', 'feature_65', 'feature_66', 'feature_67', 'feature_68', 'feature_69', 'feature_70', 'feature_71', 'feature_72', 'feature_73', 'feature_74', 'feature_75', 'feature_76', 'feature_77', 'feature_78', 'feature_79', 'feature_80', 'feature_81', 'feature_82', 'feature_83', 'feature_84', 'feature_85', 'feature_86', 'feature_87', 'feature_88', 'feature_89', 'feature_90', 'feature_91', 'feature_92', 'feature_93', 'feature_94', 'feature_95', 'feature_96', 'feature_97', 'feature_98', 'feature_99', 'feature_100', 'feature_101', 'feature_102', 'feature_103', 'feature_104', 'feature_105', 'feature_106', 'feature_107', 'feature_108', 'feature_109', 'feature_110', 'feature_111', 'feature_112', 'feature_113', 'feature_114', 'feature_115', 'feature_116', 'feature_117', 'feature_118', 'feature_119', 'feature_120', 'feature_121', 'feature_122', 'feature_123', 'feature_124', 'feature_125', 'feature_126', 'feature_127', 'feature_128', 'feature_129', 'feature_130', 'feature_131', 'feature_132', 'feature_133', 'feature_134', 'feature_135', 'feature_136', 'feature_137', 'feature_138', 'feature_139', 'feature_140', 'feature_141', 'feature_142', 'feature_143', 'feature_144', 'feature_145', 'feature_146', 'feature_147', 'feature_148', 'feature_149', 'feature_150', 'feature_151', 'feature_152', 'feature_153', 'feature_154', 'feature_155', 'feature_156', 'feature_157', 'feature_158', 'feature_159', 'feature_160', 'feature_161', 'feature_162', 'feature_163', 'feature_164', 'feature_165', 'class']\n",
            "🧹 Cleaning data...\n",
            "✅ Data cleaned: 203769 → 203769 transactions\n",
            "🔧 Preparing features...\n",
            "📊 Before filtering: 203769 samples\n",
            "📊 After filtering unknown: 46564 samples\n",
            "📊 Final class distribution: {np.int64(1): 42019, np.int64(0): 4545}\n",
            "⚖️ Class weights: [5.1225524 0.5540827]\n",
            "📐 Feature matrix shape: torch.Size([46564, 166])\n",
            "🕸️  Creating graph structure...\n",
            "Debug: Number of known txIds: 46564\n",
            "Debug: Number of known txIds overlapping with edges: 46564\n",
            "✅ Graph created: 46564 nodes, 73248 edges\n",
            "   Train: 27938, Val: 9313, Test: 9313\n",
            "🏗️  Building GCN model...\n",
            "✅ Model built:\n",
            "   Total parameters: 38,658\n",
            "   Trainable parameters: 38,658\n",
            "🏋️‍♂️ Starting training...\n",
            "Epoch 50/200, Train Loss: 0.1551, Val Loss: 0.2257\n",
            "Epoch 100/200, Train Loss: 0.0958, Val Loss: 0.2228\n",
            "Epoch 150/200, Train Loss: 0.0721, Val Loss: 0.2721\n",
            "Epoch 200/200, Train Loss: 0.0644, Val Loss: 0.2925\n",
            "✅ Training completed!\n",
            "💾 Model saved to '/content/best_gnn_model.pth'\n",
            "📊 Evaluating model...\n",
            "\n",
            "🎯 Model Performance:\n",
            "========================================\n",
            "    ACCURACY: 0.9648\n",
            "   PRECISION: 0.9906\n",
            "      RECALL: 0.9701\n",
            "          F1: 0.9803\n",
            "     ROC_AUC: 0.9766\n",
            "      PR_AUC: 0.9962\n",
            "\n",
            "🔍 Confusion Matrix:\n",
            "=========================\n",
            "True Negative:   832 | False Positive:   77\n",
            "False Negative:  251 | True Positive:  8153\n",
            "💾 Predictions saved to '/content/gnn_flagged_transactions.csv'\n",
            "\n",
            "🚨 Top 10 Most Suspicious Transactions:\n",
            "      transaction_id  true_label  illicit_probability  is_flagged\n",
            "11                51           1                  1.0           1\n",
            "5444           27119           1                  1.0           1\n",
            "8837           44118           1                  1.0           1\n",
            "1115            5849           1                  1.0           1\n",
            "1116            5854           1                  1.0           1\n",
            "4969           24854           1                  1.0           1\n",
            "4972           24866           1                  1.0           1\n",
            "5014           25047           1                  1.0           1\n",
            "4964           24839           1                  1.0           1\n",
            "8891           44382           1                  1.0           1\n",
            "📈 Creating visualizations...\n",
            "💾 Individual visualizations saved to '/content/':\n",
            "   - roc_curve.png (if applicable)\n",
            "   - precision_recall_curve.png (if applicable)\n",
            "   - confusion_matrix.png\n",
            "   - prediction_distribution.png\n",
            "   - feature_importance.png\n",
            "   - performance_metrics.png\n",
            "\n",
            "🎉 Pipeline completed successfully!\n",
            "\n",
            "Files created:\n",
            "- /content/best_gnn_model.pth\n",
            "- /content/gnn_flagged_transactions.csv\n",
            "- /content/gnn_evaluation_results.png\n",
            "\n",
            "💾 Files available for download:\n",
            "   - /content/best_gnn_model.pth (trained model)\n",
            "   - /content/gnn_flagged_transactions.csv (predictions)\n",
            "   - /content/roc_curve.png (ROC Curve, if applicable)\n",
            "   - /content/precision_recall_curve.png (Precision-Recall Curve, if applicable)\n",
            "   - /content/confusion_matrix.png (Confusion Matrix)\n",
            "   - /content/prediction_distribution.png (Prediction Distribution)\n",
            "   - /content/feature_importance.png (Feature Importance)\n",
            "   - /content/performance_metrics.png (Performance Metrics)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ROMp--tvkmnP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}